#!/bin/bash
#==========================================================
# FloodRisk-DL Memory-Optimized US Cities DEM Processing
# Memory-optimized job script for processing US cities DEM data
# This version uses conservative memory settings to prevent OOM kills
#
# (C) FloodRisk-DL Team  Jan 2025
#
# Licensed under the Apache License, Version 2.0 (the "License");
#   You may not use this file except in compliance with the License.
#   You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software distributed under the License is 
#  distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions and limitations under the License.
#==========================================================

#*** SLURM setting when needed
#SBATCH --job-name=US_CITIES_DEM_MEM_OPT
#SBATCH --time=48:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --gpus=1
#SBATCH --gpu_cmode=shared
#SBATCH --mem=256G
#SBATCH -p serc

#================================================
# (0) Basic Setting (for workstation)

##
ml python/3.12.1
ml cuda/12.6.1
export HF_HOME=/home/users/li1995/global_flood/FloodRisk-DL/terratorch
cd /home/users/li1995/global_flood/FloodRisk-DL/terratorch/meta_learning
source /home/users/li1995/global_flood/FloodRisk-DL/.venv/bin/activate

# Set base paths
BASE_DIR="/home/users/li1995/global_flood/FloodRisk-DL"
DEM_DIR="${BASE_DIR}/data/download_US_DEM/merged_dem_results"
OUTPUT_DIR="${BASE_DIR}/inference_results"
SCRIPT_DIR="${BASE_DIR}/terratorch/meta_learning"

# Path to county shapefile
COUNTY_SHAPEFILE="/home/users/li1995/global_flood/FloodRisk-DL/data/download_US_DEM/national_map_downloader/cb_2018_us_county_20m.shp"

# Create output directory if it doesn't exist
mkdir -p "${OUTPUT_DIR}"

# Function to get rainfall for a specific zarr file
get_rainfall() {
    local zarr_file=$1
    
    echo "Getting precipitation data for $zarr_file using NOAA Atlas 14..."
    
    # Use the NOAA Atlas 14 precipitation script
    rainfall_mm=$(python get_precipitation_zarr.py "$zarr_file")
    
    if [ $? -eq 0 ] && [ ! -z "$rainfall_mm" ]; then
        echo "Using rainfall: ${rainfall_mm}mm for 1-hour, 500-year return period"
        echo "$rainfall_mm"
    else
        echo "Error getting NOAA precipitation data. Using default 100mm"
        echo "100"
    fi
}

# Function to determine memory-optimized parameters based on DEM size
get_memory_params() {
    local zarr_file=$1
    
    # Get DEM dimensions using Python
    python3 << EOF
import xarray as xr
import sys

try:
    ds = xr.open_zarr("$zarr_file")
    if len(ds.data_vars) > 0:
        var_name = list(ds.data_vars.keys())[0]
        da = ds[var_name]
    else:
        da = ds
    
    height, width = da.shape
    total_pixels = height * width
    
    # Determine parameters based on size - using n_workers=4 as requested
    if total_pixels > 100_000_000:  # > 100M pixels (very large like Harris County)
        print("--window_size=256 --overlap=32 --batch_size=500000 --n_workers=4")
    elif total_pixels > 50_000_000:  # > 50M pixels (large)
        print("--window_size=384 --overlap=48 --batch_size=750000 --n_workers=4")
    elif total_pixels > 10_000_000:  # > 10M pixels (medium)
        print("--window_size=512 --overlap=64 --batch_size=1000000 --n_workers=4")
    else:  # Small datasets
        print("--window_size=512 --overlap=128 --batch_size=5000000 --n_workers=4")
        
except Exception as e:
    print(f"Error analyzing DEM: {e}", file=sys.stderr)
    # Default conservative parameters with n_workers=4
    print("--window_size=256 --overlap=32 --batch_size=500000 --n_workers=4", file=sys.stderr)
    print("--window_size=256 --overlap=32 --batch_size=500000 --n_workers=4")
EOF
}

# Process each zarr file
for zarr_file in "${DEM_DIR}"/*_cropped_dem.zarr; do
    if [ -d "$zarr_file" ]; then  # Zarr files are directories
        zarr_filename=$(basename "$zarr_file")
        echo "Processing Zarr file: $zarr_filename"
        
        # Create output filename
        output_filename="${OUTPUT_DIR}/inference_${zarr_filename%.zarr}.zarr"
        if [ -f "$output_filename" ]; then
            echo "Output file already exists, skipping: $output_filename"
            continue
        fi
        
        # Get rainfall for this location
        rainfall_result=$(get_rainfall "$zarr_file")
        rainfall_mm=$(echo "$rainfall_result" | tail -n 1)
        
        # Get memory-optimized parameters for this DEM
        echo "Analyzing DEM size for memory optimization..."
        memory_params=$(get_memory_params "$zarr_file")
        
        echo "Input: $zarr_file"
        echo "Rainfall: ${rainfall_mm}mm"
        echo "Output: $output_filename"
        echo "Memory-optimized parameters: $memory_params"
        echo ""
        
        # Run the memory-optimized inference
        cd "${SCRIPT_DIR}"
        
        # Use the memory-optimized parameters
        if python dem_inference_zarr.py \
            --zarr_file="$zarr_file" \
            --rainfall_mm="$rainfall_mm" \
            --output_file="$output_filename" \
            $memory_params; then
            echo "Successfully processed: $zarr_filename"
        else
            echo "Error processing: $zarr_filename"
        fi
        
        echo "----------------------------------------"
        echo ""
    fi
done

echo "All processing completed!" 