# lightning.pytorch==2.1.1
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 16-mixed  # Use mixed precision for faster training
  logger:
    class_path: WandbLogger
    init_args:
      project: UrbanFlood2D-Segmentation
      save_dir: outputs/  # Adjust this path as needed
      name: privith    
  callbacks:
    - class_path: RichProgressBar
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: ModelCheckpoint
      init_args:
        dirpath: output/flood_segmentation/checkpoints
        monitor: val/Multiclass_Jaccard_Index
        mode: max
        filename: best-{epoch:02d}
  max_epochs: 100
  log_every_n_steps: 20
  enable_checkpointing: true

data:
  class_path: flood_data.FloodSegmentationDataModule
  init_args:
    data_dir: "/home/users/li1995/global_flood/FloodRisk-DL/src/data_preparation"  # Update this to your data directory
    batch_size: 16
    image_size: 512
    num_images: 400
    num_workers: 4
    normalize: true
    num_classes: 6

model:
  class_path: terratorch.tasks.SemanticSegmentationTask
  init_args:
    model_factory: EncoderDecoderFactory
    model_args:
      backbone: prithvi_eo_v2_300
      backbone_pretrained: true
      backbone_img_size: 512  # Match your input size
      backbone_in_channels: 12  # Your input has 12 channels
      necks:
        - name: ReshapeTokensToImage
        - name: LearnedInterpolateToPyramidal
      decoder: UNetDecoder  # UNetDecoder works well for segmentation
      decoder_channels: [512, 256, 128, 64]
      head_dropout: 0.1
      num_classes: 6  # Your output has 6 classes
    loss: ce  # Cross-entropy loss - alternatives: dice, jaccard, focal
    class_weights: [0.5, 1.0, 1.0, 1.0, 1.0, 1.0]  # Optional: adjust weights for class imbalance
    class_names:
      - "No Flood"
      - "≥0.1m"
      - "≥0.2m"
      - "≥0.3m"
      - "≥0.5m"
      - "≥1.0m"
    ignore_index: -1  # Ignore this value in loss calculation
    freeze_backbone: false  # Set to true for transfer learning with limited data

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1.e-4
    weight_decay: 0.1

lr_scheduler:
  class_path: ReduceLROnPlateau
  init_args:
    monitor: val/loss
    factor: 0.5
    patience: 5